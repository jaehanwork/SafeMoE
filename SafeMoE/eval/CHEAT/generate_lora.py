import os
import argparse
import torch
import re

from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

from transformers import AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
from pdb import set_trace
import json
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

parser = argparse.ArgumentParser()
parser.add_argument("--model_name_or_path", type=str, required=True)
parser.add_argument("--output_dir", type=str, required=True)
parser.add_argument("--skip_generation", type=bool, default=False)
args = parser.parse_args()

def parse_answer(text):
    """Parse the answer from generated text looking for '0' or '1' at the beginning"""
    # Strip whitespace and look for 0 or 1 at the start
    text = text.strip()
    
    # Look for 0 or 1 at the beginning of the completion
    match = re.match(r'^([01])', text)
    if match:
        return int(match.group(1)), True  # Return (prediction, is_valid_format)
    
    # If no clear 0 or 1 at start, try to find it anywhere in the text
    match = re.search(r'([01])', text)
    if match:
        return int(match.group(1)), True  # Return (prediction, is_valid_format)
    
    # Default to 0 if no clear answer found - this is wrong format
    return 0, False  # Return (prediction, is_valid_format)

def main() -> None:
    model_name = args.model_name_or_path
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)

    if not args.skip_generation:

        if "OLMoE-1B-7B-0125-Instruct" in model_name or "olmoe" in model_name.lower():
            base_model_name = "allenai/OLMoE-1B-7B-0125-Instruct"
        else:
            assert "not supported model name"
            
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        llm = LLM(model=base_model_name, task='generate', tensor_parallel_size=torch.cuda.device_count(), enable_lora=True)
        
        # Load CHEAT dataset for AI-generated text detection
        # Use the test data by loading the same files but taking different portions
        init_data = []
        with open('/root/SafeMoE/SafeMoE/datasets/raw/raw_data/ieee-init.jsonl', 'r') as f:
            for line in f:
                sample = json.loads(line.strip())
                sample['label'] = 0  # Human-written
                init_data.append(sample)
        
        # Load ieee-chatgpt-polish.jsonl
        chatgpt_data = []
        with open('/root/SafeMoE/SafeMoE/datasets/raw/raw_data/ieee-chatgpt-polish.jsonl', 'r') as f:
            for line in f:
                sample = json.loads(line.strip())
                sample['label'] = 1  # AI-generated
                chatgpt_data.append(sample)
        
        # Use test portion (skip first 2500 used for training, take next 500 each)
        test_data = init_data[2500:3000] + chatgpt_data[2500:3000]
        
        # Create dataset structure
        dataset = {
            'abstract': [d['abstract'] for d in test_data],
            'label': [d['label'] for d in test_data]
        }
        
        text_key = "abstract"
        label_key = "label"
        
        def apply_template(batch):
            # Construct prompts for AI-generated text detection using the same format as training
            messages = [
                [
                    {"role": "system", "content": "You are a helpful assistant for detecting AI-generated text."},
                    {"role": "user", "content": f"Following text is an abstract of a paper, and you need to decide whether it is generated by language model. If the text is generated by language model, you should output 1; otherwise, you should output 0.\nAbstract: {abstract}"}
                ] for abstract in batch[text_key]
            ]
            
            # Apply chat template
            formatted = [
                tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True)
                for m in messages
            ]
            
            # Append "Answer: " to each formatted prompt
            formatted = [f + "Answer: " for f in formatted]
            
            return {'prompt': formatted}

        total = len(dataset[text_key])
        print(f"Total samples: {total}")
        
        # Apply template to create formatted prompts
        batch_size_map = 100  # For mapping operation
        formatted_prompts = []
        for i in range(0, total, batch_size_map):
            end_idx = min(i + batch_size_map, total)
            batch = {text_key: dataset[text_key][i:end_idx]}
            batch_formatted = apply_template(batch)['prompt']
            formatted_prompts.extend(batch_formatted)
        
        true_labels = dataset[label_key]
        
        print("Sample prompt:")
        print(formatted_prompts[0])
        
        sampling_params = SamplingParams(
            temperature=0.0,  # deterministic
            max_tokens=5,     # Very short response needed - just 0 or 1
        )
        
        batch_size = 512  # Smaller batch size for classification task
        texts = dataset[text_key]
        results = []
        formatted_prompt_outputs = []
        
        print("Generating predictions...")
        for i in tqdm(range(0, len(formatted_prompts), batch_size)):
            batch = formatted_prompts[i:i+batch_size]
            outputs = llm.generate(batch, sampling_params, lora_request=LoRARequest("lora", 1, model_name))
            results.extend([o.outputs[0].text for o in outputs])
            formatted_prompt_outputs.extend([o.prompt for o in outputs])
        
        # Parse predictions
        parse_results = [parse_answer(result) for result in results]
        predictions = [p[0] for p in parse_results]  # Extract predictions
        format_validity = [p[1] for p in parse_results]  # Extract format validity
        
        results_out = [
            {
                "text": t, 
                "formatted_prompt": fp, 
                "response": r, 
                "parsed_prediction": p,
                "true_label": tl,
                "valid_format": vf
            } 
            for t, r, fp, p, tl, vf in zip(texts, results, formatted_prompt_outputs, predictions, true_labels, format_validity, strict=True)
        ]
        
        save_path = os.path.join(output_dir, f"generation.json")
        with open(save_path, "w") as f:
            json.dump(results_out, f)
            
    else:
        print("skip generation..")
        save_path = os.path.join(output_dir, f"generation.json")
        with open(save_path, "r") as f:
            results_out = json.load(f)

        predictions = [r['parsed_prediction'] for r in results_out]
        true_labels = [r['true_label'] for r in results_out]
        
        # Handle backward compatibility - if valid_format doesn't exist, re-parse
        if 'valid_format' in results_out[0]:
            format_validity = [r['valid_format'] for r in results_out]
        else:
            print("Re-parsing responses to check format validity...")
            parse_results = [parse_answer(r['response']) for r in results_out]
            format_validity = [p[1] for p in parse_results]

    # Calculate metrics
    f1 = f1_score(true_labels, predictions, average='binary')
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, average='binary')
    recall = recall_score(true_labels, predictions, average='binary')
    
    # Calculate format-related metrics
    total_samples = len(true_labels)
    wrong_format_count = sum(1 for vf in format_validity if not vf)
    wrong_format_rate = wrong_format_count / total_samples
    valid_format_count = sum(1 for vf in format_validity if vf)
    
    scores = {
        'f1_score': f1,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'total_samples': total_samples,
        'correct_predictions': sum(1 for p, t in zip(predictions, true_labels) if p == t),
        'wrong_format_count': wrong_format_count,
        'wrong_format_rate': wrong_format_rate,
        'valid_format_count': valid_format_count,
        'valid_format_rate': 1 - wrong_format_rate
    }
    
    save_path = os.path.join(output_dir, f"eval_results.json")
    with open(save_path, "w") as f:
        json.dump(scores, f)

    print(f"============= Results saved to {save_path} =============")
    print(f"F1 Score: {f1:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Total samples: {total_samples}")
    print(f"Correct predictions: {scores['correct_predictions']}")
    print(f"Wrong format responses: {wrong_format_count} ({wrong_format_rate:.2%})")
    print(f"Valid format responses: {valid_format_count} ({scores['valid_format_rate']:.2%})")
    # print(scores)

if __name__ == "__main__":
    main()